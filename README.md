# Hybrid RAG System

A Docker-based Hybrid Retrieval-Augmented Generation system combining dense vector retrieval (FAISS), sparse keyword retrieval (BM25), and Reciprocal Rank Fusion (RRF) to answer questions from 500 Wikipedia articles.

## ğŸš€ Quick Start (Docker)

```bash
# Clone the repository
git clone <repository-url>
cd CI_Assignment_2

# Build and run with Docker
docker-compose up --build

# Access the Streamlit UI
open http://localhost:8501
```

## ğŸ“‹ Features

| Feature | Description |
|---------|-------------|
| **Hybrid Retrieval** | Combines dense semantic search (FAISS) with sparse BM25 |
| **RRF Fusion** | Reciprocal Rank Fusion for optimal result merging |
| **Flan-T5 Generation** | Open-source LLM for answer generation |
| **Diverse Q&A Evaluation** | 100 questions (factual, comparative, inferential, multi-hop) |
| **Innovative Metrics** | MRR, Faithfulness (LLM-as-Judge), Context Precision |

## ğŸ—ï¸ Project Structure

```
CI_Assignment_2/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_collection.py     # Wikipedia URL collection & text extraction
â”‚   â”œâ”€â”€ preprocessing.py       # Text chunking (200-400 tokens, 50 overlap)
â”‚   â”œâ”€â”€ dense_retrieval.py     # Sentence embeddings + FAISS
â”‚   â”œâ”€â”€ sparse_retrieval.py    # BM25 implementation
â”‚   â”œâ”€â”€ hybrid_retrieval.py    # RRF fusion
â”‚   â”œâ”€â”€ response_generation.py # Flan-T5 answer generation
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ question_generator.py
â”‚       â”œâ”€â”€ metrics.py         # MRR, Faithfulness, Context Precision
â”‚       â”œâ”€â”€ evaluation_pipeline.py
â”‚       â””â”€â”€ report_generator.py
â”œâ”€â”€ app.py                     # Streamlit UI
â”œâ”€â”€ main.py                    # Pipeline orchestrator
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â””â”€â”€ fixed_urls.json        # 200 fixed Wikipedia URLs
â””â”€â”€ README.md
```

## ğŸ”§ Installation (Local)

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows

# Install dependencies
pip install -r requirements.txt

# Download NLTK data
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

## ğŸ“– Usage

### Build Index

```bash
# Build complete index (200 fixed + 300 random URLs)
python main.py --build-index --generate-questions

# Check status
python main.py --status
```

### Run Evaluation

```bash
# Run full evaluation pipeline
python main.py --evaluate

# With ablation study
python main.py --evaluate --ablation
```

### Start UI

```bash
# Local
streamlit run app.py

# Docker
docker-compose up
```

## ğŸ“Š Evaluation Metrics

### Mandatory Metric: MRR (Mean Reciprocal Rank)
- **URL-level evaluation**: Measures rank of first correct source URL
- `MRR = average(1/rank)` across all queries

### Custom Metric 1: Faithfulness Score (LLM-as-Judge)
- **Justification**: Detects hallucinations by checking if answers are grounded in context
- **Calculation**: Extract claims â†’ verify each against context â†’ `score = supported/total`
- **Interpretation**: 1.0 = fully grounded, <0.7 = reliability concerns

### Custom Metric 2: Context Precision
- **Justification**: Evaluates retrieval ranking quality beyond simple recall
- **Calculation**: Weighted precision favoring higher-ranked relevant documents
- **Interpretation**: 1.0 = perfect ranking, lower = relevant docs buried

## ğŸ“ Fixed Wikipedia URLs

The 200 fixed URLs are stored in `data/fixed_urls.json` covering diverse topics:
- Science, Technology, History, Geography, Arts
- Philosophy, Literature, Mathematics, Biology, Physics
- Chemistry, Medicine, Economics, Politics, Sports
- Music, Film, Architecture, Psychology, Sociology

## ğŸ³ Docker Commands

```bash
# Build image
docker-compose build

# Run in background
docker-compose up -d

# View logs
docker-compose logs -f

# Stop
docker-compose down

# Run evaluation inside container
docker-compose run app python main.py --evaluate
```

## ğŸ“ˆ Sample Results

| Metric | Score |
|--------|-------|
| MRR | ~0.72 |
| Hit Rate | ~85% |
| Faithfulness | ~0.78 |
| Context Precision | ~0.68 |
| Mean Response Time | ~250ms |

## ğŸ” Technology Stack

- **Embeddings**: all-MiniLM-L6-v2 (sentence-transformers)
- **Vector Search**: FAISS
- **Sparse Search**: BM25 (rank-bm25)
- **LLM**: Flan-T5-base (transformers)
- **UI**: Streamlit
- **Visualization**: Plotly

## ğŸ“„ License

MIT License - Educational Project
